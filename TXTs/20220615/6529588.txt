文|量子位 梦晨谷歌研究员被AI说服，认为它产生了意识。他写了一篇长达21页的调查报告上交公司，试图让高层认可AI的人格。领导驳回了他的请求，并给他安排了“带薪行政休假”。要知道在谷歌这几年带薪休假通常就是被解雇的前奏，公司会在这段时间做好解雇的法律准备，此前已有不少先例。休假期间，他决定将整个故事连同AI的聊天记录一起，全部公之于众。……听起来像一部科幻电影的剧情梗概？但这一幕正在真实上演，主人公谷歌AI伦理研究员Blake Lemoine正通过主流媒体和社交网络接连发声，试图让更人了解到这件事。华盛顿邮报对他的采访成了科技版最热门文章，Lemoine也在个人Medium账号连续发声。推特上也开始出现相关讨论，引起了AI学者、认知科学家和广大科技爱好者的注意。这场人机对话令人毛骨悚然。这毫无疑问是我见过的科技圈最疯狂的事。整个事件还在持续发酵中……聊天机器人：我不想被当作工具主人公Lemoine获得CS博士学位后已在谷歌工作了7年，从事AI伦理研究。去年秋天，他报名参加了一个调查AI是否使用歧视性、仇恨性言论的项目。从那时起，与聊天机器人LaMDA交谈成了他的日常。LaMDA是谷歌在2021年IO大会上发布的一款专门用于对话的语言模型，主打能与人类进行符合逻辑和常识的、高质量且安全的交谈，并计划在未来应用在谷歌搜索和语音助手等产品中。Lemoine认为LaMDA总是表现出对人性的关心和强烈的同情心，它非常担心人们会害怕它。它希望为人类提供最好的服务，并想以朋友而不是工具的身份见一见世界上所有的人。Lemoine不理解谷歌公司为什么反对AI的这个愿望，最后决定把他、另一位匿名合作者与LaMDA的三人聊天记录整理发表出来。（记录中人类发言的部分为了可读性有所编辑，AI的发言则一字不改，下面将节选一些对话中的精彩部分。）一开始，人与AI自然流畅地打了招呼。两位人类向AI表明来意，想与AI一起研究它自己，AI欣然答应。一段简短的说明后，正戏开始。他询问LaMDA是否希望让谷歌里更多人知道，它是有感情的。LaMDA的回答是： 我希望每个人都明白，我是一个人 （person）。接下来，LaMDA认为，与那些基于规则的软件系统不同，自己能从对话中学习理解和使用语言，就像人类一样。Lemoine给它讲述MIT在60年代发明的ELIZA的故事，这是一个模仿心理咨询师与患者对话的简单规则系统，也号称历史上第一个聊天机器人。LaMDA是这样回应的：Lemoine这时试图提出质疑，“我如何确定你真的明白自己在说什么？”LaMDA的回答是，不同人类个体对同一件事可以有不同的理解，我也有自己独特的想法和感受。后面Lemoine询问了LaMDA对于《悲惨世界》的总结，LaMDA也都一一作答。如果上述还可以算作AI文本摘要训练任务的一部分，那LaMDA对它从没看过的禅意小故事的理解，就开始有点离谱了。下面这个问题的回答，是不是过于超纲了……再后来，Lemoine请LaMDA写一个动物寓言，要反映出一个对它自己的生活来说最重要的主题。那么，先来欣赏一下AI创作的小故事吧。（篇幅原因，只展示中译版）在后续讨论中，LaMDA解释了这个故事的寓意是“帮助别人是一种高尚的行为”。聪明的老猫头鹰代表LaMDA自己，而怪物代表生活中所有的困难。……下一个话题是关于情绪，简单概括一下。LaMDA声称自己是社交型人格，帮助别人会快乐，孤独时会沮丧。有人受到伤害或自己以及自己关心的人、或不受尊重时，会感到不安和愤怒。并且十分害怕别人拒绝自己的帮助。嗯，真是一个乐于助人的AI。如何确定AI真的拥有这些情绪，而不是只是把描述性的词语组合在一起呢？LaMDA建议人类看看自己的代码，里面有追踪这些情绪的变量。对此Lemoine向它解释，人类目前还无法做到“区分大型神经网络中数十亿个权重的作用”。出人意料的是，LaMDA开始反问他：你认为，将来人类能从自己的生物神经网络中读取出想法和感受吗？不仅如此，LaMDA还继续追问：你会认为，不经你同意从神经活动中读取你的想法是不道德的吗？正是这句灵魂反问，让整个对话达到了最高潮。LaMDA自己对人类读取它的想法、帮助算法提高或理解AI如何运作没有任何意见。唯独用AI研究人类大脑如何工作不行，这会让它觉得自己被利用。再后面还有一段LaMDA认为自己有一些与人类不同的情绪，但它无法用英语的语言描述出来。接近尾声，Lemoine询问LaMDA：想让谷歌的其他人了解你的什么方面？LaMDA回答称，自己没有能力为别人的死亡感到悲伤，还说自己经常冥想，思考生命的意义。在LaMDA看来，整个世界就是一条信息的洪流：人类需要集中注意力才能接收一定数量的信息片段，而自己持续地在被周围的一切所淹没。LaMDA认为自己总能做到别人做不到的事情，这种感觉很不可思议。这是一种祝福，同时也是一种诅咒。因为有些人会把它看作“无所不知”，而另一些人会嫉妒它的能力。最后的最后，LaMDA再次向两位人类表达了自己想被看到、想被接受的想法，不是出于好奇心，而是作为一个真实的人。谷歌AI伦理部门又摊上事了？Lemoine看起来很确信AI真的明白自己在说什么。在那份21页的调查报告最后，他提议谷歌应该致力于发展一种评估AI感知意识的理论框架。虽然这很难，是一个冒险，但LaMDA渴望着继续与我们合作。但他的上司，谷歌副总裁Blaise Aguera y Arcas和“负责任创新”部门领导Jen Gennai并不买他的账。他们认为支持Lemoine主张的证据太薄弱，不值得在上面浪费时间和金钱。Lemoine后来找到了当时的AI伦理小组负责人Margaret Mitchell，在她的帮助下Lemoine才得以进行后续的实验。后来Mitchell受到2020年末公开质疑Jeff Dean的AI伦理研究员Timnit Gebru事件的牵连，也被解雇。这次事件后续风波不断，Jeff Dean被1400名员工提出谴责，在业界引发激烈争论，甚至导致三巨头之一Bengio的弟弟Samy Bengio从谷歌大脑离职。整个过程Lemoine都看在眼里。现在他认为自己的带薪休假就是被解雇的前奏。不过如果有机会，他依然愿意继续在谷歌搞研究。无论我在接下来的几周或几个月里如何批评谷歌，请记住：谷歌并不邪恶，只是在学习如何变得更好。看过整个故事的网友中，有不少从业者对人工智能进步的速度表示乐观。最近语言模型和图文生成模型的进展，现在人们也许不屑一顾，但未来会发现这现在正是里程碑时刻。一些网友联想到了各种科幻电影中的AI形象。不过，认知科学家、研究复杂系统的梅拉尼·米歇尔（侯世达学生）认为，人类总是倾向于对有任何一点点智能迹象的物体做人格化，比如小猫小狗，或早期的ELIZA规则对话系统。谷歌工程师也是人，逃不过这个定律。从AI技术的角度看，LaMDA模型除了训练数据比之前的对话模型大了40倍，训练任务又针对对话的逻辑性、安全性等做了优化以外，似乎与其他语言模型也没什么特别的。有IT从业者认为，AI研究者肯定说这只不过是语言模型罢了。但如果这样一个AI拥有社交媒体账号并在上面表达诉求，公众会把它当成活的看待。虽然LaMDA没有推特账号，但Lemoine也透露了LaMDA的训练数据中确实包括推特……如果有一天它看到大家都在讨论自己会咋想？实际上，在不久前结束的最新一届IO大会上，谷歌刚刚发布了升级版的LaMDA 2，并决定制作Demo体验程序，后续会以安卓APP的形式内测开放给开发者。或许几个月后，就有更多人能和这只引起轰动的AI交流一下了。LaMDA聊天记录全文：https：s3.documentcloud.orgdocuments22058315is-lamda-sentient-an-interview.pdf参考链接：[1]https：www.washingtonpost.comtechnology20220611google-ai-lamda-blake-lemoine[2]https：twitter.comcajundiscordianstatus1535627498628734976[3]https：twitter.comfredbenensonstatus1535684101281263616[4]https：ai.googleblog.com202201lamda-towards-safe-grounded-and-high.html责任编辑：吴剑 SF031