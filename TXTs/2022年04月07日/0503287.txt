原标题：“再生”达利+机器人瓦力，文字生成图片的AI升级版来了！澎湃新闻记者 邵文这是一张AI系统DALL-E 2根据文字描述“戴着贝雷帽和穿黑色高领毛衣的柴犬”（Shiba Inu dog wearing a beret and black turtleneck）生成的图像。时隔一年，DALL-E的升级版来了！当地时间4月6日，人工智能研究机构OpenAI发布DALL-E 2（文本到图像生成程序）。DALL-E 2具有更高分辨率和更低延迟，精确度改善了71.7%，写实度改善了88.8%，解析度更是原本的4倍，还可结合概念、属性及风格打造更生动的图像，如以莫奈（Claude Monet）的风格画出草原上的狐狸。同时新增两大功能：更细颗粒度的文字局部修改图像，以及生成原图的多重风格变体。前者比如这样！在原图的2区域增加一个火烈鸟游泳圈上下分别为在原图的1区域和2区域增加一个小狗DALL-E 2在更细的层面上应用DALL-E的文本到图像的能力。用户可以从现有的图片开始，选择一个区域，并告诉模型如何来修改它。模型可以填充（或删除）物体，同时考虑到阴影方向、反射与质地等细节。后者比如这样！以同一张图像为基准，建立不同风格或编排的版本。生成的图片是1024 x 1024像素，比原始模型提供的256 x 256像素有了飞跃DALL-E的名称来自于艺术家萨尔瓦多·达利（Salvador Dalí）和《机器人总动员》的主角WALL-E，第一版于2021年1月首次亮相。DALL-E奠基在具备1750亿个参数的GPT-3模型上，但它仅使用120亿个参数，利用一个文字与图像配对的资料集，以文字叙述来产生图像。萨尔瓦多·达利（Salvador Dalí）《机器人总动员》的主角 机器人WALL-E（瓦力）OpenAI研究科学家Prafulla Dhariwal表示：“DALL-E 1只是从语言中采用了GPT-3方法，并将其应用于生成图像：我们将图像压缩成一系列单词，然后学会预测接下来的内容”。但是单词匹配并不一定能捕捉到人类认可的重点，而且预测过程限制了图像的真实性。于是用CLIP（OpenAI去年发布的计算机视觉系统）来观察图像，并以人类的方式总结它们的内容。DALL-E系统根据文字“牛油果型的扶手椅”自动创作的部分图像CLIP是原版DALL·E功能实现的基础，DALL-E 2则结合了CLIP和扩散模型两种技术的优点。DALL·E图像生成的“扩散”（diffusion）过程可以理解为从“一堆点”出发，用越来越多的细节把图像填充完整。扩散模型的特点在于，在牺牲多样性的前提下，能大大提升生成图像的逼真度。DALL-E 2根据“Teddy bears mixing sparkling chemicals as mad scientists, steampunk.”描述生成的图像为避免生成的图片被滥用，目前OpenAI已经实施了一些内置的保护措施。该模型在已剔除不良数据的数据集上进行训练，将由经过OpenAI审查的合作伙伴进行测试，用户被禁止上传或生成“非G级”和“可能造成伤害”的图像，以及任何涉及仇恨符号、裸体、猥亵手势，或“与正在发生的重大地缘政治事件有关的重大阴谋或事件”的图像。该模型也无法根据姓名生成任何可识别的人脸，即使要求的是“蒙娜丽莎”之类的内容。同时，DALL·E 2 在生成的图片上都标有水印，以表明该作品是 AI 生成的。理想情况下这些措施可以限制其产生不良内容的能力。与之前一样，该工具并未直接向公众发布。但研究人员可以提交申请预览该系统，OpenAI希望以后将DALL·E 2纳入该组织的API工具集中，使其可用于第三方应用程序。Dhariwal说道，“我们希望分阶段进行这个过程，以从获得的反馈中不断评估如何安全地发布这项技术。”责任编辑：李跃群责任编辑：王珊珊