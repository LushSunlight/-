原标题：谷歌使用AI检测用户“歧视性”词汇，被指侵犯“思想自由”澎湃新闻记者 南博一 实习生 袁佩仪谷歌在今年4月初宣布将使用人工智能来检测用户文档中的“歧视性”词汇，并建议用户用更加“包容性”的词语替换。一些倡导言论自由和隐私保护的人认为，这一功能破坏了用户的“思想自由”。据“今日俄罗斯”4月24日报道，谷歌正在使用一款新的人工智能“包容性语言助手”来检测用户语言中的“歧视性”词汇，并建议用户用更“正确”的词语替换它们。根据谷歌的描述，这一新工具会作为“辅助写作功能”的一部分，为谷歌文档用户提供更加简要生动的短语作为参考，还可以修改润色文章的语法。同时，潜在的歧视性或不恰当的语言将被标记出来，并自动弹出修改建议，“它可以让你的写作更加具有包容性，也更易被读者接受”。一些谷歌用户在社交平台推特（Twitter）上发出了谷歌助手给他们的建议截图。其中“housewife（家庭主妇）”一词被建议更换为“stay-at-home-spouse（居家配偶），“mankind（人类，也可特指人类中的男性）”被建议换成“humankind（人类总称）”，“policeman（警察，也可特指男警察）”则应改为“police officer（警察）”。除此之外，其他诸如“blacklistwhitelist（黑名单白名单）”“masterslave（主人奴隶）”等词也被列入敏感词汇。对此，英国非营利组织“Big Brother Watch”的总监卡洛在23日接受英国《每日电讯报》采访时表示，谷歌的言论监管行为令人“毛骨悚然”，并称这种技术是“侵入性的”，“它破坏了用户隐私和言论自由，更遏制了本应日益增长的思想自由”。英国学者拉迪克则认为，这一功能扼杀了个人的创造性表达。他对《每日电讯报》说，“难道所有的书面艺术作品，小说、艺术、诗歌．．．．都应当遵循一套相同的、乏味的模板吗？”此外，据美国VICE杂志19日报道，宣扬“白人至上”的美国3K党（Ku Klux Klan）前领袖大卫·杜克在采访中曾多次使用带有殖民背景的侮辱性词汇“nigger（黑鬼）”一词来指称黑人，但谷歌的“包容性语言助手”并没有对其提出任何警告。自2020年来，谷歌就要求其员工在代码和文档中使用“包容性语言”，并列出了一份“禁止词汇”清单。例如“black box testing（黑箱测试）”就被要求替换为“opaque box testing（不透明盒测试）”。谷歌发言人在接受《每日电讯报》采访时表示，目前还没能找到一个完美的解决方案，使人工智能识别尽可能减少不必要的单词联想和偏见。同时谷歌也发布声明称，人工智能技术仍在进步，并不断从人类输入中学习，“最终的目标是消除英语语言中‘所有的’偏见和歧视”。责任编辑：王蒙