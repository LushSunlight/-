3月22日，在2022 GTC大会上，NVIDIA宣布推出采用Hopper架构的新一代加速计算平台，其将取代两年前推出的Ampere架构（NVIDIA迄今为止最成功的GPU架构）。同时，NVIDIA发布了其首款基于Hopper架构的GPU—NVIDIA H100。NVIDIA H100集成了800亿个晶体管，采用台积电N4工艺，是全球范围内最大的加速器，拥有Transformer引擎和高度可扩展的NVLink互连技术（最多可连接达256个H100 GPU，相较于上一代采用HDR Quantum InfiniBand网络，带宽高出9倍，带宽速度为900GBs）等功能，可推动庞大的AI语言模型、深度推荐系统、基因组学和复杂数字孪生的发展。Hopper架构以“计算机软件工程第一夫人”Grace Hopper命名。Grace Hopper是计算机科学的先驱之一，发明了世界上第一个编译器——A-0 系统。1945年，Grace Hopper在 Mark Ⅱ中发现了一只导致机器故障的飞蛾，从此“bug” 和 “debug” （除虫） 便成为计算机领域的专用词汇。NVIDIA表示，H100 GPU在FP16、FP32和FP64计算方面比上一代A100快三倍，在8位浮点数学运算方面快六倍。“对于大型Transformer模型的训练，H100 将提供高达9倍的性能，过去需要数周时间才能完成的训练可以减少到几天内，”NVIDIA产品管理高级总监Paresh Kharya在发布会上表示。Transformer现在已成为自然语言处理的标准模型方案，也是深度学习模型领域最重要的模型之一。NVIDIA创始人兼CEO黄仁勋表示，“数据中心正在转变成‘AI工厂’，它们处理大量数据，以实现智能。NVIDIA H100是全球AI基础设施的引擎，让企业能够利用其实现自身AI业务的加速。”H100的800亿个晶体管采用了专为NVIDIA加速计算需求设计的TSMC 4N工艺，因而能够显著提升AI、HPC、显存带宽、互连和通信的速度，并能够实现近5TBs的外部互联带宽。H100是首款支持PCIe 5.0的GPU，也是首款采用HBM3的GPU，可实现3TBs的显存带宽。据介绍，20个H100 GPU便可承托相当于全球互联网的流量，使其能够实时运行数据推理的大型语言模型和推出先进的推荐系统。H100也是全球首款具有机密计算功能的加速器，可保护AI模型和正在处理的数据。据NVIDIA介绍，客户还可以将机密计算应用于医疗健康和金融服务等隐私敏感型行业的联邦学习，也可以应用于共享云基础设施。H100采用新的DPX指令可加速动态规划，适用于包括路径优化和基因组学在内的一系列算法，与CPU和上一代GPU相比，其速度提升分别可达40倍和7倍。Floyd-Warshall算法（可在动态仓库环境中为自主机器人车队寻找最优线路）与Smith-Waterman算法（可用于DNA和蛋白质分类与折叠的序列比对）也在其加速之列。H100将支持聊天机器人使用功能强大的monolithic Transformer语言模型Megatron 530B，吞吐量比上一代产品高出30倍，同时满足实时对话式AI所需的次秒级延迟。利用H100，研究人员和开发者能够训练庞大的模型，如包含3950亿个参数的混合专家模型，训练速度加速达9倍，将训练时间从几周缩短到几天。H100可部署于各种数据中心，包括内部私有云、云、混合云和边缘数据中心，产品预计于今年晚些时候全面发售。责任编辑：李昂