澎湃新闻记者 邵文在过去几年中，大规模人工智能或机器学习的限制因素首先是硬件能力，其次是复杂软件框架的可扩展性。最后一个限制与计算组件的关系不大，而与冷却处理器、加速器和存储设备有关。而随着对计算需求的持续增长，以及对削减电力成本，降低碳排放的需要，可能意味着我们要对计算系统冷却方式重新思考。《The Next Platform》在近日的一篇报道中指出，液体冷却是未来人工智能发展的下一个关键点。其认为，之所以没有更广泛地讨论这一点，是因为数据中心已经具备充足的冷却能力，通常配备空调机组和标准的冷通道、热通道实施方案。对于需要一个或两个CPU的一般企业应用程序，这是可以接受的。然而，AI训练系统不是两个CPU，而是至少有两个高端CPU和额外的四到八个 GPU。功耗从普通企业级服务器的500瓦到700瓦，到单个AI训练节点的2500瓦到4500瓦之间。想象一下这样的功耗下产生的热量，然后想象一个空调装置试图用冷空气冷却它。对于这种每机架的计算和热量密度，有一点很清楚，那就是没有办法吹出足够的空气来充分冷却地球上一些最昂贵、高性能的服务器设备。这会导致计算元素受到限制，或在极端情况下导致关闭。同时，空气是一个糟糕的热导体，比如在寒冷的冬天，人们会觉得裸露在室外的金属块比空气更寒冷，这是因为金属是良好的热导体，它比空气从你手中带走的热量要多。同时，相比水来说，空气也无法容纳大量的热。这时就要考虑另一个因素：服务器机架密度。疫情期间，由于企业居家办公、学校远程教育及“健康码”等大量应用，数据需求量暴增。据《Wealth Management》的报道，数据中心的机房需求处于历史最高水平，最大化密度的需求正在推动新的服务器创新，但冷却只能通过在机架（可以驻留更多系统的地方）留出空隙来让空气保持跟上。在这些情况下，空气冷却不足以完成任务，同时会导致每个机架的计算量减少，服务器机房空间浪费更多。对于在双CPU服务器上具有单核作业的普通企业系统，问题可能不会很快复杂化。但是对于密集的AI训练集群，需要大量的能量来引入冷空气，在后端捕获热量，并将其恢复到合理的温度。这种消耗远远超出了为系统本身供电所需的消耗。那么液体冷却如何呢？联想 HPC（High Performance Computing，高性能计算）和AI欧洲、中东和非洲地区总监Noam Rosen解释道，“当你使用温水、室温水来散热来冷却组件时，你不需要冷却任何东西，无需投入能源来降低水温。当你获得进行大规模AI训练的国家实验室和数据中心的节点数时，这将变得非常重要。”Rosen在《NLP中深度学习的能量和政策考虑因素》（Energy and Policy Considerations for Deep Learning in NLP）中，通过对几种常见大型AI模型的训练进行生命周期评估，指出定量细节以比较一般企业机架级电源需求与AI训练所需的电源需求。他们检查了自然语言处理（NLP）的模型训练过程，发现NLP训练过程可以排放数百吨碳，相当于一辆普通汽车整个寿命周期排放量的近五倍。“从头开始训练新模型或将模型应用于新数据集时，由于调整现有模型所需的持续时间和计算能力，该过程会排放更多的碳。因此，研究人员建议行业和企业齐心协力，使用更高效、运行所需能源更少的硬件。”Rosen表示。Rosen具体比较了温水冷却与空气冷却，“今天，可以在一个机架上安装一百多个Nvidia A100 GPU，但唯一的方法是用温水冷却。相同的密度在风冷机架中是不可能的，因为所有空插槽都可以让空气冷却组件，即便如此，它也可能无法解决那么多GPU产生的热量。”根据服务器配置，温水冷却可以带走85%到95%的热量。Rosen解释道，由于水的允许入口温度达45°C，在许多情况下，不需要耗能的冷水机，这意味着更大的节约、更低的总成本和更少的碳排放。用水来冷却并不新鲜，它在大型主机业务中的使用已经历时几十年。但随着大型主机在数据中心中的应用慢慢减少，水冷也随之被淘汰。但目前，由于空气已经不足以冷却高功率密度设备，水或者说是液体冷却再次被提起，并被视为未来冷却数据中心的关键。责任编辑：张亚楠